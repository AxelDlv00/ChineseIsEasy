{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f27c9e40",
   "metadata": {},
   "source": [
    "# ChineseIsEasy - Generation of Categories, Examples and Explanations for Anki Cards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209639fb",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7347f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "from pycccedict.cccedict import CcCedict\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5838c74",
   "metadata": {},
   "source": [
    "## General Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3893e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_api_key = Path('secrets/api_key.txt')\n",
    "path_excel = Path('data/SUBTLEX-CH-WF.xlsx')\n",
    "path_prompt_cat = Path('prompts/prompt_categorie.txt')\n",
    "path_prompt_ex = Path('prompts/prompt_exemples.txt')\n",
    "path_prompt_exp = Path('prompts/prompt_explications.txt')\n",
    "path_return = Path('generated_data/words_with_categories.parquet')\n",
    "batch_input_file = Path('batch_input.jsonl')\n",
    "\n",
    "MAX_WORDS = 2500 # If you use batches, the number of tokens per request is quite low so we need to limit the number of words per batch\n",
    "\n",
    "api_key = path_api_key.read_text().strip()\n",
    "client = OpenAI(api_key=api_key)\n",
    "cccedict = CcCedict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bbdd91",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8981e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_mot(mot: str):\n",
    "    \"\"\"Retourne un dict avec les infos d’un mot chinois via CCCEDICT.\"\"\"\n",
    "    entries = cccedict.get_entries()\n",
    "    resultats = [e for e in entries if e[\"simplified\"] == mot or e[\"traditional\"] == mot]\n",
    "    if not resultats:\n",
    "        return {\"simplifie\": mot, \"traditionnel\": \"\", \"pinyin\": [], \"sens\": [\"[introuvable]\"]}\n",
    "    pinyins = list({e[\"pinyin\"] for e in resultats})\n",
    "    sens = []\n",
    "    for e in resultats:\n",
    "        sens.extend(e[\"definitions\"])\n",
    "    sens = list(dict.fromkeys(sens))\n",
    "    return {\n",
    "        \"simplifie\": resultats[0][\"simplified\"],\n",
    "        \"traditionnel\": resultats[0][\"traditional\"],\n",
    "        \"pinyin\": pinyins,\n",
    "        \"sens\": sens\n",
    "    }\n",
    "\n",
    "def numero_vers_accent(pinyin_num: str) -> str:\n",
    "    accents = {\n",
    "        'a': ['ā', 'á', 'ǎ', 'à'],\n",
    "        'e': ['ē', 'é', 'ě', 'è'],\n",
    "        'i': ['ī', 'í', 'ǐ', 'ì'],\n",
    "        'o': ['ō', 'ó', 'ǒ', 'ò'],\n",
    "        'u': ['ū', 'ú', 'ǔ', 'ù'],\n",
    "        'ü': ['ǖ', 'ǘ', 'ǚ', 'ǜ']\n",
    "    }\n",
    "    def convertir_syllabe(s):\n",
    "        if not s or not s[-1].isdigit(): return s\n",
    "        ton = int(s[-1])\n",
    "        base = s[:-1]\n",
    "        if ton < 1 or ton > 4: return base\n",
    "        for v in \"a o e i u ü\".split():\n",
    "            if v in base:\n",
    "                return base.replace(v, accents[v][ton - 1], 1)\n",
    "        return base\n",
    "    return \" \".join(convertir_syllabe(s) for s in pinyin_num.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109afad4",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac03951",
   "metadata": {},
   "outputs": [],
   "source": [
    "xls = pd.read_excel(path_excel)\n",
    "df_existing = pd.read_parquet(path_return) if path_return.exists() else pd.DataFrame(columns=[\"Word\"])\n",
    "xls = xls[~xls['Word'].isin(df_existing['Word'])].head(MAX_WORDS)\n",
    "\n",
    "# Ajouter les infos linguistiques\n",
    "xls['infos'] = xls['Word'].apply(analyse_mot)\n",
    "xls['Traditionnel'] = xls['infos'].apply(lambda x: x[\"traditionnel\"])\n",
    "xls['Pinyin'] = xls['infos'].apply(lambda x: \"; \".join(numero_vers_accent(p) for p in x[\"pinyin\"]))\n",
    "xls['Signification'] = xls['infos'].apply(lambda x: \"; \".join(x[\"sens\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389504ec",
   "metadata": {},
   "source": [
    "## Prompts Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabae83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions_cat = path_prompt_cat.read_text().strip()\n",
    "instructions_ex = path_prompt_ex.read_text().strip()\n",
    "instructions_exp = path_prompt_exp.read_text().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0419b70",
   "metadata": {},
   "source": [
    "## Generation of the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83997a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(batch_input_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for mot in xls[\"Word\"]:\n",
    "        prompts = {\n",
    "            \"cat\": instructions_cat,\n",
    "            \"ex\": instructions_ex,\n",
    "            \"exp\": instructions_exp.replace(\"AREMPLACER\", mot)\n",
    "        }\n",
    "        for typ, instr in prompts.items():\n",
    "            custom_id = f\"{mot}_{typ}\"\n",
    "            body = {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"Tu es un assistant spécialisé en chinois.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"{instr}\\n{mot}\\n\\n\"}\n",
    "                ],\n",
    "                \"temperature\": 0.7,\n",
    "            }\n",
    "            req = {\n",
    "                \"custom_id\": custom_id,\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": body\n",
    "            }\n",
    "            f.write(json.dumps(req, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Fichier batch généré : {batch_input_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c69b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_obj = client.files.create(\n",
    "    file=open(\"batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "print(\"File ID:\", file_obj.id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id=file_obj.id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\"\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "print(\"Statut initial :\", batch.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf9396a",
   "metadata": {},
   "source": [
    "## To run later (once the batch is processed by OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a773f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifie l’état et télécharge les résultats une fois complété :\n",
    "batch_status = client.batches.retrieve(batch.id)\n",
    "if batch_status.status == \"completed\":\n",
    "    output = client.files.content(batch_status.output_file_id).text\n",
    "    with open(\"batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(output)\n",
    "    print(\"Résultats enregistrés dans batch_output.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a27a8e3",
   "metadata": {},
   "source": [
    "## Results Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5401f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "if Path(\"batch_output.jsonl\").exists():\n",
    "    with open(\"batch_output.jsonl\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            cid = obj[\"custom_id\"]\n",
    "            try:\n",
    "                content = obj[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "                results[cid] = content.strip()\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Erreur pour {cid}: {e}\")\n",
    "                results[cid] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b98e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplir les colonnes avec les résultats GPT\n",
    "xls[\"Catégorie\"]   = xls[\"Word\"].map(lambda w: results.get(f\"{w}_cat\", \"\"))\n",
    "xls[\"Exemples\"]    = xls[\"Word\"].map(lambda w: results.get(f\"{w}_ex\", \"\"))\n",
    "xls[\"Explication\"] = xls[\"Word\"].map(lambda w: results.get(f\"{w}_exp\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c244359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionner avec les données existantes\n",
    "df_final = pd.concat([df_existing, xls], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dce66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder\n",
    "df_final.to_parquet(path_return, index=False)\n",
    "print(\"Résultats fusionnés dans\", path_return)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
