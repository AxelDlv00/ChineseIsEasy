{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5deb75ed",
   "metadata": {},
   "source": [
    "# Génération d'un paquet pour les mots du HSK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e7a869",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a79f143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import genanki\n",
    "\n",
    "try:\n",
    "    from pypinyin import lazy_pinyin, Style\n",
    "    USE_PYPINYIN = True\n",
    "except Exception:\n",
    "    USE_PYPINYIN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2242d89e",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "86af405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HSK_FILE = [Path(\"data/hsk3.csv\"), Path(\"data/hsk5.csv\")]\n",
    "WORDS_FILE = Path(\"../AnkiWords/generated_data/words_with_categories.parquet\")\n",
    "\n",
    "MEDIA_STROKES_DIR = Path(\"../AnkiWords/data/media/data\")\n",
    "FONT_FILE = Path(\"../AnkiWords/data/media/FZKai.ttf\")\n",
    "HANZI_JS_FILE = Path(\"../AnkiWords/data/media/_hanzi-writer.min.js\")\n",
    "SHARED_JS_PATH = Path(\"../MinimalExample/js/_shared_hanzi.js\")\n",
    "\n",
    "OUTPUT_APKG = \"ChineseIsEasy-HSK.apkg\"\n",
    "\n",
    "CHUNK_SIZE = 100\n",
    "DECK_ROOT_C2P = \"ChineseIsEasy-HSK::Caractere→PinyinSignification\"\n",
    "DECK_ROOT_P2C = \"ChineseIsEasy-HSK::PinyinSignification→Caractere\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f13ca06",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "96a359b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_id_from_key(key: str) -> int:\n",
    "    return int(hashlib.sha1(key.encode()).hexdigest()[:10], 16)\n",
    "\n",
    "def stable_guid(key: str) -> str:\n",
    "    return hashlib.sha1(key.encode()).hexdigest()\n",
    "\n",
    "def safe_str(x, default=\"\"):\n",
    "    try:\n",
    "        if pd.isna(x): return default\n",
    "        s = str(x)\n",
    "        return s if s.lower() != \"nan\" else default\n",
    "    except:\n",
    "        return default\n",
    "\n",
    "def read_any_table(path: Path) -> pd.DataFrame:\n",
    "    ext = path.suffix.lower()\n",
    "    if ext == \".csv\":\n",
    "        return pd.read_csv(path, sep=\";\")\n",
    "    if ext in (\".tsv\", \".tab\"):\n",
    "        return pd.read_csv(path, sep=\"\\t\")\n",
    "    if ext == \".parquet\":\n",
    "        return pd.read_parquet(path)\n",
    "    if ext in (\".xlsx\", \".xls\"):\n",
    "        return pd.read_excel(path)\n",
    "    raise ValueError(f\"Unsupported extension: {ext}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d9980b",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0a8223fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rich = pd.read_parquet(WORDS_FILE)\n",
    "\n",
    "for col in [\"Word\", \"Traditionnel\", \"Pinyin\", \"Signification\", \"Exemples\"]:\n",
    "    if col not in df_rich.columns:\n",
    "        df_rich[col] = \"\"\n",
    "\n",
    "# Load HSK datasets\n",
    "df_hsk_raw = pd.concat([read_any_table(f) for f in HSK_FILE], ignore_index=True)\n",
    "df_hsk = df_hsk_raw.rename(columns={\n",
    "    \"Mot\": \"Word\",\n",
    "    \"Exemple\": \"ExemplesBase\",\n",
    "    \"例句\": \"ExemplesBase\",\n",
    "    \"HSK\": \"HSK\",\n",
    "    \"Chapitre\": \"Chapitre\"\n",
    "})\n",
    "\n",
    "for col in [\"Word\", \"Pinyin\", \"Signification\", \"ExemplesBase\", \"HSK\", \"Chapitre\"]:\n",
    "    if col not in df_hsk.columns:\n",
    "        df_hsk[col] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "98b272b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_int_or_str(v):\n",
    "    try:\n",
    "        if pd.isna(v) or v == \"\":\n",
    "            return \"0\"\n",
    "        return int(float(v))\n",
    "    except:\n",
    "        return safe_str(v, \"0\")\n",
    "\n",
    "df_hsk[\"HSK\"] = df_hsk[\"HSK\"].apply(to_int_or_str)\n",
    "df_hsk[\"Chapitre\"] = df_hsk[\"Chapitre\"].apply(to_int_or_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6e14e7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PYPINYIN:\n",
    "    def fill_pinyin_if_missing(row):\n",
    "        if safe_str(row[\"Pinyin\"]) == \"\":\n",
    "            chars = safe_str(row[\"Word\"])\n",
    "            if chars:\n",
    "                return \" \".join(lazy_pinyin(chars, style=Style.TONE3)).replace(\"u:\", \"ü\")\n",
    "        return row[\"Pinyin\"]\n",
    "    df_hsk[\"Pinyin\"] = df_hsk.apply(fill_pinyin_if_missing, axis=1)\n",
    "\n",
    "df_rich_unique = df_rich.drop_duplicates(subset=[\"Word\"], keep=\"first\")\n",
    "\n",
    "df_enriched = df_hsk.merge(\n",
    "    df_rich_unique[[\"Word\", \"Traditionnel\", \"Pinyin\", \"Signification\", \"Exemples\", \"Explication\"]].rename(columns={\n",
    "        \"Pinyin\": \"Pinyin_rich\",\n",
    "        \"Signification\": \"Signification_rich\",\n",
    "        \"Exemples\": \"Exemples_rich\",\n",
    "    }), on=\"Word\", how=\"left\"\n",
    ")\n",
    "\n",
    "def choose(base, rich):\n",
    "    base = safe_str(base)\n",
    "    rich = safe_str(rich)\n",
    "    return base if base else rich\n",
    "\n",
    "df_enriched[\"Pinyin_final\"] = df_enriched.apply(lambda r: choose(r[\"Pinyin\"], r[\"Pinyin_rich\"]), axis=1)\n",
    "df_enriched[\"Signif_final\"] = df_enriched.apply(lambda r: choose(r[\"Signification\"], r[\"Signification_rich\"]), axis=1)\n",
    "df_enriched[\"Traditionnel_final\"] = df_enriched[\"Traditionnel\"].apply(safe_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "18fe7436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_examples(a, b):\n",
    "    a = safe_str(a)\n",
    "    b = safe_str(b)\n",
    "    if a and b:\n",
    "        return a + \"\\n\\n\" + b\n",
    "    return a or b\n",
    "\n",
    "df_enriched[\"Exemples_final\"] = df_enriched.apply(\n",
    "    lambda r: join_examples(r[\"ExemplesBase\"], r[\"Exemples_rich\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_enriched[\"Word\"] = df_enriched[\"Word\"].apply(safe_str)\n",
    "df_enriched = df_enriched[df_enriched[\"Word\"] != \"\"]\n",
    "df_enriched = df_enriched[df_enriched[\"Signif_final\"] != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d88d76",
   "metadata": {},
   "source": [
    "## Préparation médias (strokes + font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fa9cb7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITER_HTML = r\"\"\"\n",
    "<div id=\"writer-wrapper\" style=\"display:flex; flex-direction:column; align-items:center; gap:12px;\">\n",
    "\n",
    "  <div id=\"writer-container\"\n",
    "       style=\"display:flex; gap:20px; justify-content:center; flex-wrap:nowrap;\">\n",
    "  </div>\n",
    "\n",
    "  <button id=\"replay-btn\"\n",
    "          style=\"\n",
    "            background:#444cf7;\n",
    "            color:white;\n",
    "            border:none;\n",
    "            padding:8px 22px;\n",
    "            font-size:16px;\n",
    "            border-radius:20px;\n",
    "            cursor:pointer;\n",
    "            transition:0.2s;\n",
    "          \">Rejouer</button>\n",
    "\n",
    "  <div id=\"hanzi-data\" style=\"display:none;\">{{Characters}}</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<script src=\"_hanzi-writer.min.js\"></script>\n",
    "<script src=\"_shared_hanzi.js\"></script>\n",
    "\n",
    "<script>\n",
    "console.log(\"[Template] Calling initHanziWriter('{{Characters}}')\");\n",
    "initHanziWriter(\"{{Characters}}\");\n",
    "</script>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac401b8",
   "metadata": {},
   "source": [
    "## Construction des decks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "91e350e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_char_to_pinyin():\n",
    "    return genanki.Model(\n",
    "        model_id=stable_id_from_key(\"model_char_to_pinyin\"),\n",
    "        name=\"HSK - Char→Pinyin\",\n",
    "        fields=[\n",
    "            {\"name\": \"Characters\"},\n",
    "            {\"name\": \"Traditionnel\"},\n",
    "            {\"name\": \"Pinyin\"},\n",
    "            {\"name\": \"Signification\"},\n",
    "            {\"name\": \"Exemples\"},\n",
    "        ],\n",
    "        templates=[{\n",
    "            \"name\": \"Char→Pinyin\",\n",
    "            \"qfmt\": WRITER_HTML,\n",
    "            \"afmt\": r\"\"\"\n",
    "{{FrontSide}}<hr>\n",
    "<div style=\"font-size:32px;\"><b>{{Characters}}</b></div>\n",
    "<div style=\"font-size:30px; margin-top:10px;\">{{Pinyin}}</div>\n",
    "<div style=\"font-size:28px; margin-top:10px;\">{{Signification}}</div>\n",
    "<div style=\"white-space:pre-line; color:gray; font-size:24px; margin-top:10px;\">{{Exemples}}</div>\n",
    "\"\"\"\n",
    "        }]\n",
    "    )\n",
    "\n",
    "def build_model_pinyin_to_char():\n",
    "    return genanki.Model(\n",
    "        model_id=stable_id_from_key(\"model_pinyin_to_char\"),\n",
    "        name=\"HSK - Pinyin→Char\",\n",
    "        fields=[\n",
    "            {\"name\": \"Characters\"},\n",
    "            {\"name\": \"Traditionnel\"},\n",
    "            {\"name\": \"Pinyin\"},\n",
    "            {\"name\": \"Signification\"},\n",
    "            {\"name\": \"Exemples\"},\n",
    "        ],\n",
    "        templates=[{\n",
    "            \"name\": \"Pinyin→Char\",\n",
    "\n",
    "            \"qfmt\": r\"\"\"\n",
    "<div style=\"font-size:30px;\"><b>{{Pinyin}}</b></div>\n",
    "<div style=\"font-size:24px;\">{{Signification}}</div>\n",
    "\"\"\",\n",
    "\n",
    "            \"afmt\": r\"\"\"\n",
    "{{FrontSide}}<hr>\n",
    "\n",
    "<!-- Affichage du caractère -->\n",
    "<div style=\"font-size:32px; margin-top:10px;\"><b>{{Characters}}</b></div>\n",
    "\n",
    "<!-- Exemples -->\n",
    "<div style=\"white-space:pre-line; color:gray; font-size:24px; margin-top:10px;\">{{Exemples}}</div>\n",
    "\n",
    "<!-- Animation HanziWriter -->\n",
    "\"\"\" + WRITER_HTML + \"\"\"\n",
    "\"\"\"\n",
    "        }]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a7aa1",
   "metadata": {},
   "source": [
    "## Écriture du package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8d30f893",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_char = [p.name.replace(\".json\", \"\") for p in MEDIA_STROKES_DIR.glob(\"*.json\")]\n",
    "\n",
    "def to_valid_char(s: str) -> str:\n",
    "    return \"\".join([c for c in s if c in valid_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fcdc679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decks_hsk(df: pd.DataFrame, model_c2p, model_p2c):\n",
    "    decks = []\n",
    "\n",
    "    df_sorted = df.sort_values(by=[\"HSK\", \"Chapitre\", \"Word\"], kind=\"mergesort\")\n",
    "\n",
    "    for (hsk, chap), df_grp in df_sorted.groupby([\"HSK\", \"Chapitre\"], dropna=False):\n",
    "        n = len(df_grp)\n",
    "        n_chunks = math.ceil(n / CHUNK_SIZE)\n",
    "\n",
    "        for i in range(n_chunks):\n",
    "            chunk = df_grp.iloc[i*CHUNK_SIZE:(i+1)*CHUNK_SIZE]\n",
    "            name = f\"{i*CHUNK_SIZE:03d}-{(i+1)*CHUNK_SIZE-1:03d}\"\n",
    "\n",
    "            deck1 = genanki.Deck(\n",
    "                stable_id_from_key(f\"c2p_{hsk}_{chap}_{i}\"),\n",
    "                f\"{DECK_ROOT_C2P}::HSK{hsk}::Chapitre{chap}::{name}\"\n",
    "            )\n",
    "            deck2 = genanki.Deck(\n",
    "                stable_id_from_key(f\"p2c_{hsk}_{chap}_{i}\"),\n",
    "                f\"{DECK_ROOT_P2C}::HSK{hsk}::Chapitre{chap}::{name}\"\n",
    "            )\n",
    "\n",
    "            for _, r in chunk.iterrows():\n",
    "                word = to_valid_char(safe_str(r[\"Word\"]))\n",
    "                trad = safe_str(r[\"Traditionnel_final\"])\n",
    "                pinyin = safe_str(r[\"Pinyin_final\"])\n",
    "                signif = safe_str(r[\"Signif_final\"])\n",
    "                ex = safe_str(r[\"Exemples_final\"])\n",
    "                exp = safe_str(r[\"Explication\"])\n",
    "\n",
    "                if exp:\n",
    "                    ex = f\"<i style='color: gray;'>{exp}</i>\\n\\n{ex}\"\n",
    "\n",
    "                chars = word  # NO SPACES anymore\n",
    "\n",
    "                note1 = genanki.Note(\n",
    "                    model=model_c2p,\n",
    "                    fields=[chars, trad, pinyin, signif, ex],\n",
    "                    guid=stable_guid(f\"{chars}_c2p_{hsk}_{chap}_{i}\")\n",
    "                )\n",
    "                note2 = genanki.Note(\n",
    "                    model=model_p2c,\n",
    "                    fields=[chars, trad, pinyin, signif, ex],\n",
    "                    guid=stable_guid(f\"{chars}_p2c_{hsk}_{chap}_{i}\")\n",
    "                )\n",
    "\n",
    "                deck1.add_note(note1)\n",
    "                deck2.add_note(note2)\n",
    "\n",
    "            decks.append(deck1)\n",
    "            decks.append(deck2)\n",
    "\n",
    "    return decks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6b6f3a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hsk_package():\n",
    "    model_c2p = build_model_char_to_pinyin()\n",
    "    model_p2c = build_model_pinyin_to_char()\n",
    "\n",
    "    decks = build_decks_hsk(df_enriched, model_c2p, model_p2c)\n",
    "\n",
    "    package = genanki.Package(decks)\n",
    "    package.media_files = []\n",
    "\n",
    "    if FONT_FILE.exists():\n",
    "        package.media_files.append(str(FONT_FILE))\n",
    "\n",
    "    package.media_files += [str(p) for p in MEDIA_STROKES_DIR.glob(\"*.json\")]\n",
    "\n",
    "    package.media_files.append(str(HANZI_JS_FILE))\n",
    "    package.media_files.append(str(SHARED_JS_PATH))\n",
    "\n",
    "    package.write_to_file(OUTPUT_APKG)\n",
    "\n",
    "    print(f\"✔ Paquet généré : {OUTPUT_APKG}\")\n",
    "    print(f\"✔ Decks : {len(decks)}\")\n",
    "    print(f\"✔ Items : {len(df_enriched)}\")\n",
    "    print(f\"✔ JS partagé : {SHARED_JS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de1474",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b83e8e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Paquet généré : ChineseIsEasy-HSK.apkg\n",
      "✔ Decks : 14\n",
      "✔ Items : 273\n",
      "✔ JS partagé : ../MinimalExample/js/_shared_hanzi.js\n"
     ]
    }
   ],
   "source": [
    "build_hsk_package()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChineseIsEasy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
