{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5deb75ed",
   "metadata": {},
   "source": [
    "# Génération d'un paquet anki word-examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e7a869",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a79f143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import genanki\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea13ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pypinyin import lazy_pinyin, Style\n",
    "    USE_PYPINYIN = True\n",
    "except Exception:\n",
    "    USE_PYPINYIN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2242d89e",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "123165c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Entrées ---\n",
    "HSK_FILE = [Path(\"data/hsk3.csv\"), Path(\"data/hsk5.csv\")]        \n",
    "WORDS_FILE = Path(\"../AnkiWords/generated_data/words_with_categories.parquet\")\n",
    "\n",
    "# --- Médias ---\n",
    "MEDIA_DIR = Path(\"../AnkiWords/data/media/data\")                             # JSON des strokes (un fichier par caractère)\n",
    "FONT_FILE = Path(\"../AnkiWords/data/media/FZKai.ttf\")\n",
    "HANZI_JS_FILE = Path(\"../AnkiWords/data/media/hanzi-writer.min.js\")\n",
    "\n",
    "# --- Sortie ---\n",
    "OUTPUT_APKG = \"DictHSK.apkg\"\n",
    "\n",
    "# --- Divers ---\n",
    "CHUNK_SIZE = 100          # nombre de cartes par sous-deck\n",
    "DECK_ROOT_C2P = \"ChineseIsEasy-HSK::Caractere→PinyinSignification\"\n",
    "DECK_ROOT_P2C = \"ChineseIsEasy-HSK::PinyinSignification→Caractere\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f13ca06",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a359b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_id_from_key(key: str) -> int:\n",
    "    h = hashlib.sha1(key.encode(\"utf-8\")).hexdigest()\n",
    "    return int(h[:10], 16)\n",
    "\n",
    "def stable_guid(key: str) -> str:\n",
    "    return hashlib.sha1(key.encode(\"utf-8\")).hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0faf08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_any_table(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Lit CSV/TSV/Parquet/XLSX selon l'extension.\"\"\"\n",
    "    ext = path.suffix.lower()\n",
    "    if ext == \".csv\":\n",
    "        return pd.read_csv(path, sep=\";\")\n",
    "    if ext in (\".tsv\", \".tab\"):\n",
    "        return pd.read_csv(path, sep=\"\\t\")\n",
    "    if ext == \".parquet\":\n",
    "        return pd.read_parquet(path)\n",
    "    if ext in (\".xlsx\", \".xls\"):\n",
    "        return pd.read_excel(path)\n",
    "    raise ValueError(f\"Extension non supportée: {ext} pour {path}\")\n",
    "\n",
    "def safe_str(x, default=\"\"):\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return default\n",
    "        s = str(x)\n",
    "        return s if s.lower() != \"nan\" else default\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def load_hanzi_js(js_path: Path) -> str:\n",
    "    return js_path.read_text(encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d9980b",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a8223fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset riche\n",
    "df_rich = pd.read_parquet(WORDS_FILE)\n",
    "# colonnes attendues: Word, Traditionnel, Pinyin, Signification, Exemples\n",
    "df_rich = df_rich.copy()\n",
    "for col in [\"Word\", \"Traditionnel\", \"Pinyin\", \"Signification\", \"Exemples\"]:\n",
    "    if col not in df_rich.columns:\n",
    "        df_rich[col] = \"\"\n",
    "\n",
    "# Dataset HSK\n",
    "df_hsk_raw = pd.concat([read_any_table(file) for file in HSK_FILE], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2a3b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation noms de colonnes \n",
    "rename_map = {\n",
    "    \"Mot\": \"Word\",\n",
    "    \"Exemple\": \"ExemplesBase\",\n",
    "    \"例句\": \"ExemplesBase\",  \n",
    "    \"HSK\": \"HSK\",\n",
    "    \"Chapitre\": \"Chapitre\",\n",
    "    \"Pinyin\": \"Pinyin\",\n",
    "    \"Signification\": \"Signification\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed17c455",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hsk = df_hsk_raw.rename(columns=rename_map)\n",
    "\n",
    "# Colonnes minimales\n",
    "for col in [\"Word\", \"Pinyin\", \"Signification\", \"ExemplesBase\", \"HSK\", \"Chapitre\"]:\n",
    "    if col not in df_hsk.columns:\n",
    "        df_hsk[col] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98b272b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast pour HSK/Chapitre \n",
    "def to_int_or_str(v):\n",
    "    try:\n",
    "        if pd.isna(v) or v == \"\":\n",
    "            return \"0\"\n",
    "        iv = int(float(v))\n",
    "        return iv\n",
    "    except Exception:\n",
    "        return safe_str(v, \"0\")\n",
    "\n",
    "df_hsk[\"HSK\"] = df_hsk[\"HSK\"].apply(to_int_or_str)\n",
    "df_hsk[\"Chapitre\"] = df_hsk[\"Chapitre\"].apply(to_int_or_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e14e7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si pinyin absent dans HSK et disponible via pypinyin, on le recalcule\n",
    "if USE_PYPINYIN:\n",
    "    def fill_pinyin_if_missing(row):\n",
    "        if safe_str(row[\"Pinyin\"]) == \"\":\n",
    "            chars = safe_str(row[\"Word\"])\n",
    "            if chars:\n",
    "                # pinyin avec tons (marques) collés par espace\n",
    "                return \" \".join(lazy_pinyin(chars, style=Style.TONE3)).replace(\"u:\", \"ü\")\n",
    "        return row[\"Pinyin\"]\n",
    "    df_hsk[\"Pinyin\"] = df_hsk.apply(fill_pinyin_if_missing, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "912b34c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusion enrichie sur Word\n",
    "df_rich_unique = df_rich.drop_duplicates(subset=[\"Word\"], keep=\"first\")\n",
    "df_enriched = df_hsk.merge(\n",
    "    df_rich_unique[[\"Word\", \"Traditionnel\", \"Pinyin\", \"Signification\", \"Exemples\"]]\n",
    "        .rename(columns={\n",
    "            \"Pinyin\": \"Pinyin_rich\",\n",
    "            \"Signification\": \"Signification_rich\",\n",
    "            \"Exemples\": \"Exemples_rich\",\n",
    "        }),\n",
    "    on=\"Word\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0122728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préférence des champs: HSK d’abord, sinon dataset riche\n",
    "def choose(base, rich):\n",
    "    b = safe_str(base)\n",
    "    r = safe_str(rich)\n",
    "    return b if b else r\n",
    "\n",
    "df_enriched[\"Pinyin_final\"] = df_enriched.apply(lambda r: choose(r[\"Pinyin\"], r[\"Pinyin_rich\"]), axis=1)\n",
    "df_enriched[\"Signif_final\"] = df_enriched.apply(lambda r: choose(r[\"Signification\"], r[\"Signification_rich\"]), axis=1)\n",
    "df_enriched[\"Traditionnel_final\"] = df_enriched[\"Traditionnel\"].apply(safe_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18fe7436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat des exemples (base puis rich, avec double interligne)\n",
    "def join_examples(ex1, ex2):\n",
    "    ex1 = safe_str(ex1)\n",
    "    ex2 = safe_str(ex2)\n",
    "    if ex1 and ex2:\n",
    "        return f\"{ex1}\\n\\n{ex2}\"\n",
    "    return ex1 or ex2\n",
    "\n",
    "df_enriched[\"Exemples_final\"] = df_enriched.apply(\n",
    "    lambda r: join_examples(r[\"ExemplesBase\"], r[\"Exemples_rich\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Nettoyage minimal / filtres utiles\n",
    "df_enriched[\"Word\"] = df_enriched[\"Word\"].apply(safe_str)\n",
    "df_enriched = df_enriched[df_enriched[\"Word\"] != \"\"]\n",
    "df_enriched = df_enriched[df_enriched[\"Signif_final\"] != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d88d76",
   "metadata": {},
   "source": [
    "## Préparation médias (strokes + font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa9cb7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "media_files = [str(FONT_FILE)]\n",
    "# Ajout de tous les JSON trouvés (ça accélère la dispo des strokes hors génération à la volée)\n",
    "if MEDIA_DIR.exists():\n",
    "    media_files += [str(p) for p in MEDIA_DIR.glob(\"*.json\")]\n",
    "\n",
    "hanzi_js = load_hanzi_js(HANZI_JS_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cb290e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expression régulière pour détecter les caractères chinois (bloc CJK unifié)\n",
    "CHINESE_CHAR_RE = re.compile(r'[\\u4e00-\\u9fff]')\n",
    "\n",
    "def stroke_json_for_word(word: str) -> str:\n",
    "    \"\"\"Construit une liste JSON de datas HanziWriter (une entrée par caractère chinois uniquement).\"\"\"\n",
    "    items = []\n",
    "\n",
    "    for ch in word:\n",
    "        # Ignorer tout caractère non chinois (lettres, chiffres, ponctuation, etc.)\n",
    "        if not CHINESE_CHAR_RE.match(ch):\n",
    "            continue\n",
    "\n",
    "        jf = MEDIA_DIR / f\"{ch}.json\"\n",
    "        if jf.exists():\n",
    "            try:\n",
    "                items.append(jf.read_text(encoding=\"utf-8\"))\n",
    "            except Exception:\n",
    "                items.append(\"{}\")\n",
    "        else:\n",
    "            items.append(\"{}\")\n",
    "\n",
    "    # Si aucun caractère chinois valide n’a été trouvé, renvoyer une liste vide\n",
    "    if not items:\n",
    "        items.append(\"{}\")\n",
    "\n",
    "    return \"[\" + \",\".join(items) + \"]\"\n",
    "\n",
    "\n",
    "# ===================== Modèles =====================\n",
    "def build_model_char_to_pinyin():\n",
    "    return genanki.Model(\n",
    "        model_id=stable_id_from_key(\"model_hsk_char_to_pinyin\"),\n",
    "        name=\"HSK - Caractère→Pinyin/Signification\",\n",
    "        fields=[\n",
    "            {\"name\": \"Word\"},\n",
    "            {\"name\": \"Traditionnel\"},\n",
    "            {\"name\": \"Pinyin\"},\n",
    "            {\"name\": \"Signification\"},\n",
    "            {\"name\": \"Exemples\"},\n",
    "            {\"name\": \"StrokeJSON\"},\n",
    "        ],\n",
    "        templates=[{\n",
    "            \"name\": \"Caractère→Pinyin/Signification\",\n",
    "            \"qfmt\": r\"\"\"\n",
    "<div id=\"writer-container\" style=\"display:flex; justify-content:center; gap:10px;\"></div>\n",
    "\n",
    "<span id=\"replay-btn\" style=\"color: gray; cursor: pointer; font-size:14px; margin-top:10px; display:inline-block;\">\n",
    "  Rejouer\n",
    "</span>\n",
    "\n",
    "<script>\n",
    "\"\"\" + hanzi_js + r\"\"\"\n",
    "\n",
    "var strokes = JSON.parse(`{{StrokeJSON}}`);\n",
    "var container = document.getElementById(\"writer-container\");\n",
    "var writers = [];\n",
    "\n",
    "strokes.forEach(function(data, idx) {\n",
    "  var div = document.createElement(\"div\");\n",
    "  div.id = \"writer_\"+idx;\n",
    "  div.style.width = \"120px\";\n",
    "  div.style.height = \"120px\";\n",
    "  container.appendChild(div);\n",
    "\n",
    "  var writer = HanziWriter.create(div.id, '', {\n",
    "    width: 120, height: 120, padding: 5,\n",
    "    strokeAnimationSpeed: 1, delayBetweenStrokes: 300,\n",
    "    charDataLoader: function(c, onComplete) { onComplete(data); }\n",
    "  });\n",
    "  writer.animateCharacter();\n",
    "  writers.push(writer);\n",
    "});\n",
    "\n",
    "document.getElementById(\"replay-btn\").addEventListener(\"click\", function() {\n",
    "  writers.forEach(function(writer) {\n",
    "    writer.hideCharacter();\n",
    "    writer.showCharacter();\n",
    "    writer.animateCharacter();\n",
    "  });\n",
    "});\n",
    "</script>\n",
    "\"\"\",\n",
    "            \"afmt\": r\"\"\"\n",
    "{{FrontSide}}<hr id=\"answer\">\n",
    "\n",
    "<div style=\"font-size: 36px; margin-top: 10px;\">\n",
    "  <b>{{Word}} {{#Traditionnel}}({{Traditionnel}}){{/Traditionnel}}</b>\n",
    "</div>\n",
    "<div style=\"font-size: 34px; margin-top: 10px;\"><b>{{Pinyin}}</b></div>\n",
    "<div style=\"font-size: 30px; margin-top: 10px;\">{{Signification}}</div>\n",
    "<div style=\"font-size: 26px; margin-top: 15px; color: gray; white-space: pre-line;\">{{Exemples}}</div>\n",
    "\"\"\"\n",
    "        }]\n",
    "    )\n",
    "\n",
    "def build_model_pinyin_to_char():\n",
    "    return genanki.Model(\n",
    "        model_id=stable_id_from_key(\"model_hsk_pinyin_to_char\"),\n",
    "        name=\"HSK - Pinyin/Signification→Caractère\",\n",
    "        fields=[\n",
    "            {\"name\": \"Word\"},\n",
    "            {\"name\": \"Traditionnel\"},\n",
    "            {\"name\": \"Pinyin\"},\n",
    "            {\"name\": \"Signification\"},\n",
    "            {\"name\": \"Exemples\"},\n",
    "            {\"name\": \"StrokeJSON\"},\n",
    "        ],\n",
    "        templates=[{\n",
    "            \"name\": \"Pinyin/Signification→Caractère\",\n",
    "            \"qfmt\": r\"\"\"\n",
    "<div style=\"font-size: 24px;\"><b>{{Pinyin}}</b></div>\n",
    "<div style=\"font-size: 20px; margin-top: 10px;\">{{Signification}}</div>\n",
    "\"\"\",\n",
    "            \"afmt\": r\"\"\"\n",
    "{{FrontSide}}<hr id=\"answer\">\n",
    "\n",
    "<div id=\"writer-container\" style=\"display:flex; justify-content:center; gap:10px; margin-top:15px;\"></div>\n",
    "\n",
    "<span id=\"replay-btn\" style=\"color: gray; cursor: pointer; font-size:14px; margin-top:10px; display:inline-block;\">\n",
    "  Rejouer\n",
    "</span>\n",
    "\n",
    "<script>\n",
    "\"\"\" + hanzi_js + r\"\"\"\n",
    "\n",
    "var strokes = JSON.parse(`{{StrokeJSON}}`);\n",
    "var container = document.getElementById(\"writer-container\");\n",
    "var writers = [];\n",
    "\n",
    "strokes.forEach(function(data, idx) {\n",
    "  var div = document.createElement(\"div\");\n",
    "  div.id = \"writer_\"+idx;\n",
    "  div.style.width = \"120px\";\n",
    "  div.style.height = \"120px\";\n",
    "  container.appendChild(div);\n",
    "\n",
    "  var writer = HanziWriter.create(div.id, '', {\n",
    "    width: 120, height: 120, padding: 5,\n",
    "    strokeAnimationSpeed: 1, delayBetweenStrokes: 300,\n",
    "    charDataLoader: function(c, onComplete) { onComplete(data); }\n",
    "  });\n",
    "  writer.animateCharacter();\n",
    "  writers.push(writer);\n",
    "});\n",
    "\n",
    "document.getElementById(\"replay-btn\").addEventListener(\"click\", function() {\n",
    "  writers.forEach(function(writer) {\n",
    "    writer.hideCharacter();\n",
    "    writer.showCharacter();\n",
    "    writer.animateCharacter();\n",
    "  });\n",
    "});\n",
    "</script>\n",
    "\n",
    "<div style=\"font-size: 36px; margin-top: 10px;\">\n",
    "  <b>{{Word}} {{#Traditionnel}}({{Traditionnel}}){{/Traditionnel}}</b>\n",
    "</div>\n",
    "<div style=\"font-size: 26px; margin-top: 15px; color: gray; white-space: pre-line;\">{{Exemples}}</div>\n",
    "\"\"\"\n",
    "        }]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac401b8",
   "metadata": {},
   "source": [
    "## Construction des decks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91e350e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decks_hsk(df: pd.DataFrame,\n",
    "                    model_c2p: genanki.Model,\n",
    "                    model_p2c: genanki.Model) -> list[genanki.Deck]:\n",
    "    \"\"\"\n",
    "    Construit des decks organisés par HSK -> Chapitre, puis en tranches de CHUNK_SIZE cartes.\n",
    "    \"\"\"\n",
    "    decks = []\n",
    "\n",
    "    # tri stable: HSK croissant, Chapitre croissant, puis Word\n",
    "    df_sorted = df.sort_values(by=[\"HSK\", \"Chapitre\", \"Word\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    for (hsk, chap), df_grp in df_sorted.groupby([\"HSK\", \"Chapitre\"], dropna=False):\n",
    "        # tranche en sous-decks de taille fixe\n",
    "        n = len(df_grp)\n",
    "        n_chunks = math.ceil(n / CHUNK_SIZE)\n",
    "\n",
    "        for i in range(n_chunks):\n",
    "            chunk = df_grp.iloc[i*CHUNK_SIZE:(i+1)*CHUNK_SIZE]\n",
    "            tranche_name = f\"{i*CHUNK_SIZE:03d}-{(i+1)*CHUNK_SIZE-1:03d}\"\n",
    "\n",
    "            deck1 = genanki.Deck(\n",
    "                stable_id_from_key(f\"hsk_c2p_{hsk}_{chap}_{i}\"),\n",
    "                f\"{DECK_ROOT_C2P}::HSK{hsk}::Chapitre{chap}::{tranche_name}\",\n",
    "            )\n",
    "            deck2 = genanki.Deck(\n",
    "                stable_id_from_key(f\"hsk_p2c_{hsk}_{chap}_{i}\"),\n",
    "                f\"{DECK_ROOT_P2C}::HSK{hsk}::Chapitre{chap}::{tranche_name}\",\n",
    "            )\n",
    "\n",
    "            for _, r in chunk.iterrows():\n",
    "                word = safe_str(r[\"Word\"])\n",
    "                trad = safe_str(r[\"Traditionnel_final\"])\n",
    "                pinyin_final = safe_str(r[\"Pinyin_final\"])\n",
    "                signif_final = safe_str(r[\"Signif_final\"])\n",
    "                ex_final = safe_str(r[\"Exemples_final\"])\n",
    "\n",
    "                # Strokes (liste JSON par caractère)\n",
    "                stroke_json = stroke_json_for_word(word)\n",
    "\n",
    "                note1 = genanki.Note(\n",
    "                    model=model_c2p,\n",
    "                    fields=[word, trad, pinyin_final, signif_final, ex_final, stroke_json],\n",
    "                    guid=stable_guid(f\"{word}_c2p_hsk{hsk}_c{chap}_chunk{i}\")\n",
    "                )\n",
    "                note2 = genanki.Note(\n",
    "                    model=model_p2c,\n",
    "                    fields=[word, trad, pinyin_final, signif_final, ex_final, stroke_json],\n",
    "                    guid=stable_guid(f\"{word}_p2c_hsk{hsk}_c{chap}_chunk{i}\")\n",
    "                )\n",
    "\n",
    "                deck1.add_note(note1)\n",
    "                deck2.add_note(note2)\n",
    "\n",
    "            decks.extend([deck1, deck2])\n",
    "\n",
    "    return decks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a7aa1",
   "metadata": {},
   "source": [
    "## Écriture du package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcdc679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hsk_package():\n",
    "    model_c2p = build_model_char_to_pinyin()\n",
    "    model_p2c = build_model_pinyin_to_char()\n",
    "\n",
    "    decks = build_decks_hsk(df_enriched, model_c2p, model_p2c)\n",
    "\n",
    "    package = genanki.Package(decks)\n",
    "    package.media_files = []\n",
    "    # ajoute police si présente\n",
    "    if FONT_FILE.exists():\n",
    "        package.media_files.append(str(FONT_FILE))\n",
    "    # ajoute tous les JSON de strokes présents\n",
    "    if MEDIA_DIR.exists():\n",
    "        package.media_files += [str(p) for p in MEDIA_DIR.glob(\"*.json\")]\n",
    "\n",
    "    package.write_to_file(OUTPUT_APKG)\n",
    "    print(f\"Paquet généré : {OUTPUT_APKG}  —  {len(decks)} sous-decks, {len(df_enriched)} entrées.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de1474",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b6f3a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paquet généré : DictHSK.apkg  —  10 sous-decks, 234 entrées.\n"
     ]
    }
   ],
   "source": [
    "build_hsk_package()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChineseIsEasy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
